#+TITLE: BEEF.ME:

This project is a experimental Voice Tweet Like application.

* Milestones:

** DONE : Frontend Setup: [2/2]
 1. [X] Setup Routing
 2. [X] Setup Page structure
** TODO : Create User [1/3]
 1. [X] Backend - Open Endpoint to Create
 2. [ ] Backend - Register Success on Kafka
 3. [ ] Frontend - Write Registration Page
** TODO : Create Beef [1/3]
 1. [X] Backend - Open Endpoint to create Beef
 2. [ ] Frontend - Create component to create Beef
 3. [ ] Backend - Register Success on Kafka
** TODO : Follow User [0/2]
 1. [ ] Backend - Create Endpoint to follow User
 2. [ ] Frontend - Add Follow Button and Functionality
** TODO : Create Feed [0/5]
 1. [ ] Infrastructure - Design how Feed Works
    Assuming you're talking about a news feed in the context of a social network you need to anticipate the scaling challenges, i.e. each user's feed is unique and feeds are real-time. The 'naive' approach is to keep an event log for all entries, on read you find a users friends and then look up recent activity for each friend in the event log. This leads to lots of scans and doesn't scale well.

    You can use an alternative architecture where instead of constructing the feed on read you do it on write, effectively keeping an 'inbox' for each feed consumer, piping any new event to the appropriate inboxes. This structure would contain the ids of the entries that need to be in the feed, with further information about the entry stored once in another store. As news feeds tend to focus on recent events this inbox can often be capped in size, in effect a capped queue, where a new event pushes out the oldest event. This makes it a good candidate for Redis as the space requirements are calculable, writing an event to multiple inboxes is not a concern as you're only writing to memory and not taxing your disks.

    A capped queue is trivial to implement using Redis' list structure, reading the contents of your feed is then an O(1) single key lookup from memory which is hard to beat. The meta-data for each entry would be stored in a persisted store like Mongo.

    You presumably still want the event data persisted so I would write a single entry to your global event log into Mongo each time an entry is created. If you need to construct a new feed (e.g. for a new user) you can use the slower scan approach to construct the Redis queue, from which point you maintain it during writes. If you capped your Redis queue you may also need to revert to scanning the event log for users who page down their feed beyond the oldest entry in your memory based inbox. As long as these scans only occur for a small percentage of users then it should scale pretty well. The Mongo event log also gives you the possibility to completely rebuild the Redis data in a disaster situation.

    You could also consider setting an expiration on the queue keys, renewing them on read such that queues that are never used expire and keep the in memory data size down. You need to weight that against the cost of rebuilding the queue if it's expired.
 2. [ ] Backend - Open Endpoint to Request Feed
 3. [ ] Backend - Design how Feed Updates
 4. [ ] Frontend - Design Page
 5. [ ] Frontend - Design Infinite scroll
